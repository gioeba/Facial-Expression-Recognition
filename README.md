
# Facial-Expression-Recognition
პროექტის ლინკი: https://wandb.ai/gioeba-free-university-of-tbilisi-/facial-expression-recognition?nw=nwusergioeba
## Preprocessing
პირველ რიგში, განვიხილოთ ის მიდგომები რაც გამოყენებულია მონაცემთა დამუშავებისთვის, სანამ მათზე ნეირონულ ქსელს გავწვრთნით. მოცემული მონაცემები გავყავი 70-15-15 პროპორციით ტრენინგ, ვალიდაციის და ტესტ სეტებად. Preprocessing ის ძირითადი შემადგენელი ნაწილია თითოეული პიქსელისთვის მისი საშუალო მნიშვნელობის გამოთვლა და სატრენინგო სეტში ყოველი ელემენტისთვის გამოკლება. რადგან პიქსელების მნიშვნელობები 0-დან 1-მდეა და შესაბამისად, ცენტრი 0-ში არ აქვთ, ამით 0-ის მიმართ გავცენტრავთ, რაც უკეთეს აქტივაციებს და უფრო სწრაფ კონვერგენციას შეუწყობს ხელს. 
ასევე, როგორც შემდგომ, მოდელის ქსელის სტრუქტურის განხილვისასაც ვნახავთ, მთავარი პრობლემა იყო overfit ის პრობლემის მოგვარება, რადგან საკმაოდ დიდხანს ვალიდაციის და ტრენინგის შედეგებს შორის ~30%იანი სვაობა იყო. ამ პრობლემის მოგვარებაში ერთ-ერთი მთავარი ნაბიჯი იყო, მონაცემთა ოგმენტაცია, მათი შემთხვევითი მოდიფიკაციები იმგვარად, რათა მოდელს მონაცემთა დიდი ვარიაცია ჰქონდეს თავდაპირველის გარდა და არ დაიზეპიროს. ამის მაგალითებია სხვადასხვა ალბათობებით სურათის ჰორიზონტალური არეკვლა, -10-დან 10 გრადუსამდე მოტრიალება, გაუსური ხმაურის დამატება და ა.შ.
## ქსელის სტრუქტურა
წონების ინიციალიზაციისთვის ქსელში ვიყენებთ Xavier Initailization-ს, რათა გრადიენტის ჩაქრობისთვის ხელი შეგვეშალა და ინიცალიზაცია ყოველ გაშვებაზე კონსისტენტური ყოფილიყო. ასევე სწრაფი კონვერგენციისთვის ვიყენებ Batch Normalization სა და Adam optimization-ს. Layer normalization და არანაირი ნორმალიზაციის გატესტვის შემდეგ, რაც wandb ზე დალოგილ რანებშიც ჩანს, აშკარაა, რომ batch normalization, დანარჩენი ორიგან განსხვავებით, უკეთ უწყობს ხელს დასწავლას და სწრაფ კონვერგენციას. Adam Optimization ასევე ლოკალური მინიმუმის სწრაფად პოვნაში საუკეთესოა momentum და RMSProp თან შედარებით, რადგან ფაქტობრივად, აერთიანებს ამ ორს. ასევე იგივე მიზეზს ემსახურება learning rate scheduler, რომელიც ნელ-ნელა ამცირებს learning-rate ს უკეთესი კონვერგენციისთვის. გამოვცადე batch-ების სხვადასხვა ზომა 64-დან 512-მდე,64 overfit-ს ხელს უწყობდა, 512 512-ზე დასწავლასვერ ახერხებდა, 256 იდეალური გამოდგა.
რაც შეეხება თავად ნეირონული ქსელის სტრუქტურას, თავდაპირველად შევქმენი ნეირონული ქსელი ორი კონვოლუციური შრით, 2 MaxPooling შრით და ReLU აქტივაციის ფუნქციით, რის შედეგსაც გვაქჩვენებს ყველაზე ძველი, StarterCNN wandb-ზე. ამ მოდელმა დაბალი სიზუსტე აჩვენა ტრენინგზეცდა ვალიდაციაშიც, რითიც აშკარა გახდა მოდელის კომპლექსურობის გაზრდის აუცილებლობა, ამიტომ დავამატე კიდევ ერთი კონვოლუცირი და MaxPooling შრე, ასევე აქტივაციის ფუნქციად გამოვიყენე LeakyReLU, გრადიენტის გაქრობის პრობლემის უკეთესი მოგვარების გამო. მოდელის კომპლექსურობის გაზრდის შემდეგი, ტრენინგისას მოდელის სიზუსტე გახდა 99%-მდე, ხოლო ვალიდაციის სეტზე მოდელი 67% ზე მეტ სიზუსტეს ვერდებდა, რაც აშკარა overfit არის და აქედან დაწყებული, დანარჩენი ყველა run ემსახურება ამ overfit ის გასწორებას.
ამაში ძალიან დიდი წვლილი შეიტანა Data Augmentation ის დამატება და ნელ-ნელა გართულებამ. თუმცა ასევე გავტესტე ჰიპერპარამეტრთა ბევრი სხვადასხვა კომბინაცია. ვიყენებთ Dropout=0.3-ს, რაც overfit ს ამცირებს და ტრენინგის დროს ამცირებს. მისი გაზრდა ვცადე 0.5 და 0.7მდე overfit ის მოსაგვარებლად, თუმცა აშკარა გახდა, რომ ეს ჩარევა ძალიან უხეში იყო და მოდელი რეალურად დასწავლას ვერ ახერხებდა. ამაზე უკეთესი მიდგომა აღმოჩნდა 2-განზომილებიანი Dropout ჩამატება ყოველი კონვოლუციური და MaxPooling შრის შემდეგ.
თავდაპირველად, ტრენინგი მიდიოდა 10 ეპოქა, სხვადასხვა ჰიპერპარამეტრთა კომბინაციისთვის კომბინაციებისთვის გამოვცადე სხვადასხვა ხანგრძლივობა, ხოლო Augmentation ის საბოლოოდ დახვეწის და overfit ის გასწორების შემდეგ, აშკარა აღმოჩნდა, რომ მოდელი 10 ეპოქაში ვერ ახერხებდა თავისი პოტენციალის სრულად გამოვლენას, ამიტომ ეპოქების რაოდენობა გავზარდე 30-მდე. საბოლოო ჯამში მივიღეთ 61% სიზუსტე საწვრთნელ სეტზე და 57% ვალიდაციაზე, რაც განზოგადების საკმაოდ კარგი მაჩვენებელია, რასაც ადასტრებს სატესტოდ გადანახულ მონაცემებზე ვალიდაციის ანალოგიური მაჩვენებელი Final_CNN run-ზე.

